<!DOCTYPE html>
<html><head><meta charset="utf-8"><script src="main.js" type="text/javascript"></script><script type="text/javascript">tailrecursion.hoplon.app_pages._index_DOT_html.hoploninit();</script><link rel="stylesheet" href="custom.css"></head><body><div class="reveal"><link rel="stylesheet" href="reveal.js/css/reveal.css"><link id="theme" rel="stylesheet" href="reveal.js/css/theme/simple.css"><link rel="stylesheet" href="reveal.js/plugin/highlight/github.min.css"><div class="slides"><section><h1 class="slide-title intro">Factorization Techniques for Recommendation</h1><div>Paul English</div><div><a shape="rect" href="mailto:paul.english@utah.edu">paul.english@utah.edu</a></div><div><a shape="rect" href="https://github.com/log0ymxm">https://github.com/log0ymxm</a></div><aside class="notes">Several topics are covered:
What is a RecSys?
What is factorization?
How is matrix factorization done?
How can it accommodate additional data?
What are some improvements in it's model?</aside></section><section><section><h2 class="slide-title chapter">What is Recommendation?</h2></section><section><h3 class="slide-title slide">Some Obvious Examples</h3><ul><li>Netflix suggesting movies you might like after you've added some ratings</li><li>Amazon offering "commonly sold together" discount packages</li><li>Facebook showing "friends you might know"</li></ul><aside class="notes">There are a ton of ways recommendation is used
in every day software systems. These are a few trivial and easily recognized
applications of it. It's a beneficial tool that can be applied in varying ways.</aside></section><section><h3 class="slide-title slide">We can look at different aspects of the data</h3><ul><li><strong>Content Based Filtering: </strong>Makes use of attributes, e.g. Pandora.</li><li><strong>Collaborative Filtering: </strong>Makes use of past user behavior, e.g. Netfix ratings, Amazon purchases.</li></ul><aside class="notes">There are various ways we can approach
 the RecSys problem. We can look at characteristics of our data,
or we can look at how users and their peers interact with data.</aside></section><section><h3 class="slide-title slide">We can think of recommendation problems as a graph or network</h3><div>Multiple vertices with edges connecting them.</div><img src="img/graphs/Alemdar@Alemdar.gif" width="260" height="260"><img src="img/graphs/Grund@poli_large.APlusAT.gif" width="260" height="260"><img src="img/graphs/QCD@conf5_0-4x4-10.gif" width="260" height="260"><aside class="notes">Graphs are a very simple way to think of a recommendation
problem. We represent items as nodes in a graph, and relation information as
vertices. These graphs are often weighted, often bipartite, and often very sparse.</aside></section><section><h3 class="slide-title slide">They are seen in all kinds of industries</h3><ul><li>Social Networks</li><li>Communication Networks</li><li>Road Networks</li><li>Product Networks</li><li>...</li></ul><aside class="notes">Networks are seen in all kinds of interesting industries.
It's very common for datasets to be viewable as a graph, and if there is a benefit
in finding probably connections in these graphs these techniques can be used.</aside></section><section><h3 class="slide-title slide">There are different ways to solve the collaborative problem</h3><ul><li>Item Similarity / Nearest Neighbors</li><li>Factorization and Latent Dimensions</li></ul><aside class="notes">We can take a few different approaches to solving
RecSys problems. Nearest neighbors or item similarity approach. Matrix factorization
approach. They have different strengths, they can work well in tandem, and
they aren't necessarily the only way to solve this kind of issue.</aside></section></section><section><section><h2 class="slide-title chapter">Matrix Factorization</h2></section><section><h3 class="slide-title slide">Matrix Factorization is the process
of splitting (factoring) one matrix into two</h3><br clear="none"><img class="no-border" src="img/r-approx-p-dot-q.png"><br clear="none"><img class="no-border" src="img/r-p-q-k.png"><aside class="notes">We factor a matrix by
finding two matrices with appropriate dimension that multiply
together to approximate. R is observed data. P &amp; Q are what we're trying to find.</aside></section><section><h3 class="slide-title slide">You can think of it like pulling apart a matrix</h3><img class="no-border" src="img/matrix-multiplication-split.png"></section><section><h3 class="slide-title slide">This lets us capture the relationships between each item and user</h3><div style="position:relative;width:100%;height:400px;"><img class="fragment no-border current-visible" src="img/factor-relations/factor-relations-step-1.png" style="position:absolute;left:0;top:0;" width="100%"><img class="fragment no-border" src="img/factor-relations/factor-relations-step-2.png" style="position:absolute;left:0;top:0;" width="100%"><img class="fragment no-border" src="img/factor-relations/factor-relations-step-3.png" style="position:absolute;left:0;top:0;" width="100%"><img class="fragment no-border" src="img/factor-relations/factor-relations-step-4.png" style="position:absolute;left:0;top:0;" width="100%"><img class="fragment no-border" src="img/factor-relations/factor-relations-step-5.png" style="position:absolute;left:0;top:0;" width="100%"><img class="fragment no-border" src="img/factor-relations/factor-relations-step-cross.png" style="position:absolute;left:0;top:0;" width="100%"></div></section><section><h3 class="slide-title slide">We choose a cost function that we can minimize.</h3><div>Regularized Square Error</div><br clear="none"><img class="no-border" src="img/regularized-square-error.png"><aside class="notes">We choose a function that we can use to measure cost.
Regularized square error is shown here, though any cost function appropriate
can be used. We find the difference here between R and R-hat (P dot Q). We add the
l2 norm to regularize the plane we will be searching over.</aside></section><section><h3 class="slide-title slide">We find our factor matrices using an optimization routine</h3><div>e.g. gradient descent</div><img class="no-border" src="img/gradient-descent/DisplayVelocityPlotOverContourPlotExample_02.png"><aside class="notes">We optimize our cost function using a technique like gradient descent.
ALS, SGD, MCMC, and other methods work as well.</aside></section></section><section><section><h2 class="slide-title chapter">Building on the Matrix Factorization Model</h2></section><section><h3 class="slide-title slide">Accounting for bias</h3><div>Some users in general rate higher or lower. Some movies
get better reviews. We can account for this bias. Our new parameters b_i and b_u
represent a deviation from the mean.</div><br clear="none"><img class="no-border" src="img/mf-bias.png"><aside class="notes">We look for bias parameters that can help even out recommendations
between our users. We include a global bias as well.</aside></section><section><h3 class="slide-title slide">Including Additional Information</h3><div>Maybe we want a hybrid content/collaborative approach. For example: movie descriptions,
genre, actors &amp; directors involved, user age.</div><br clear="none"><img class="no-border" src="img/mf-additional-information.png"><aside class="notes">We can include infomration about our items,
or about our users, or implicit feedback we get in the behaviour of our
system.</aside></section><section><h3 class="slide-title slide">Temporal Dynamics</h3><div>Paying attention to time is useful. Maybe a users ratings change
over time. Maybe various genres go into and out of popularity.</div><br clear="none"><img class="no-border" src="img/mf-temporal-dynamics.png"><aside class="notes"></aside></section><section><h3 class="slide-title slide">Confidence Intervals</h3><div>We can use confidence intervals to account somewhat for uncertainty.</div><br clear="none"><img class="no-border" src="img/mf-confidence-interval.png"><aside class="notes"></aside></section></section><section><section><h2 class="slide-title chapter">The Factorization Machine</h2></section><section><h3 class="slide-title slide">What is this?</h3><img src="img/lego-car-seankenney.jpg"><div class="fragment"><p><strike>car</strike></p><p>legos!</p></div><aside class="notes"></aside></section><section><h3 class="slide-title slide">The Factorization Machine.</h3><br clear="none"><ul><li>Matrix factorization is a specialized case of the factorization machine.</li><li>It's a generalized learning algorithm, thus it's much more flexible.</li><li>It's great at dealing with sparse data, so it continues to perform well for collaborative filtering problems.</li></ul><aside class="notes"></aside></section><section><h3 class="slide-title slide">The FM Model</h3><br clear="none"><img class="no-border" src="img/2-degree-factorization-machine.png"><aside class="notes"></aside></section><section><h3 class="slide-title slide">The n-degree FM Model</h3><br clear="none"><img class="no-border" src="img/n-degree-factorization-machine.png"><aside class="notes"></aside></section><section><h3 class="slide-title slide">We Change the Data Representation</h3><div>The FM is a general learner, so we have to adjust the way we represent our data.</div><br clear="none"><img class="no-border" src="img/structuring-data-for-the-fm.png" width="900"><aside class="notes"></aside></section></section><section><section><h2 class="slide-title chapter">Questions?</h2></section><section><h3 class="slide-title slide">Thanks!</h3><div>Paul English</div><div><a shape="rect" href="mailto:paul.english@utah.edu">paul.english@utah.edu</a></div><div><a shape="rect" href="https://github.com/log0ymxm">https://github.com/log0ymxm</a></div></section><section><h3 class="slide-title slide">References</h3><div style="font-size:0.3em;"><h4>Reading &amp; Images</h4><ol><li>Rendle, “Factorization Machines with libFM,” 2012.</li><li>Rendle, et.al., “Fast context-aware recommendations with factorization machines,” 2011.</li><li>Freudenthaler, et.al., “Bayesian factorization machines,” 2011.</li><li>Adams, et.al., “Incorporating Side Information in Probabilistic Matrix Factorization with Gaussian Processes,” 2010.</li><li>Rendle, “Factorization machines,” 2010.</li><li>Schmidt, et.al., “Bayesian Non-negative Matrix Factorization,” 2009.</li><li>Koren, et.al., “Matrix factorization techniques for recommender systems,” 2009.</li><li>Salakhutdinov and Mnih, “Bayesian probabilistic matrix factorization using Markov chain Monte Carlo,” 2008.</li><li>Salakhutdinov and Mnih, “Probabilistic Matrix Factorization.,” 2007.</li><li><a shape="rect" href="http://www.cise.ufl.edu/research/sparse/matrices/">http://www.cise.ufl.edu/research/sparse/matrices/</a></li><li><a shape="rect" href="http://snap.stanford.edu/data/index.html">http://snap.stanford.edu/data/index.html</a></li><li><a shape="rect" href="http://www.mathworks.com/help/matlab/creating_plots/display-quiver-plot-over-contour-plot.html">http://www.mathworks.com/help/matlab/creating_plots/display-quiver-plot-over-contour-plot.html</a></li><li><a shape="rect" href="http://seankenney.com/portfolio/mazda/">http://seankenney.com/portfolio/mazda/</a></li></ol></div></section></section></div></div></body></html>